{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b145ddec-c532-4950-90e1-0c7ddd8d9a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications import ResNet50,DenseNet121,EfficientNetB3\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import io\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from matplotlib import image as mpimg\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Conv2D,MaxPool2D,Flatten,Dropout,BatchNormalization, Dropout\n",
    "from sklearn.metrics import confusion_matrix , classification_report, ConfusionMatrixDisplay\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f07c055e-3abe-4d67-9050-66b8b410c946",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = r\"D:\\CAPSTONE 400A\\final_dataset\\Tuberculosis\\TB_Chest_Radiography_Database\\split\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f614383f-d553-4611-8386-d8fc2ff6b68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (224, 224)\n",
    "batch_size = 32\n",
    "num_classes = len(os.listdir(os.path.join(m, 'train')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5443b71-34ec-46ea-b0a9-ec032133fa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255.0\n",
    " \n",
    ")\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
    "test_datagen = ImageDataGenerator(rescale=1.0 / 255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bfb49d6-b921-4bec-abb7-26a8096becd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = EfficientNetB3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab0b8f82-a5b4-421e-a875-ce1889aa3d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " efficientnetb3 (Functional  (None, 7, 7, 1536)        10783535  \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 7, 7, 75)          1036875   \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 7, 7, 75)          300       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 4, 4, 75)          0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 4, 4, 50)          33800     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4, 4, 50)          0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 4, 4, 50)          200       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 2, 2, 50)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 2, 2, 25)          11275     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 2, 2, 25)          100       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 1, 1, 25)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               13312     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11880423 (45.32 MB)\n",
      "Trainable params: 11792820 (44.99 MB)\n",
      "Non-trainable params: 87603 (342.20 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "\n",
    "model.add(Conv2D(75, (3, 3), strides=1, padding=\"same\", activation=\"relu\", input_shape=(224, 224, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2, 2), strides=2, padding=\"same\"))\n",
    "\n",
    "model.add(Conv2D(50, (3, 3), strides=1, padding=\"same\", activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2, 2), strides=2, padding=\"same\"))\n",
    "\n",
    "model.add(Conv2D(25, (3, 3), strides=1, padding=\"same\", activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2, 2), strides=2, padding=\"same\"))\n",
    "\n",
    "# Flatten the output\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add dense layers\n",
    "model.add(Dense(units=512, activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=2, activation=\"softmax\"))  # Updated to match the number of classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecafdeb5-2ab2-4147-b7ab-36bd2f4f36d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91299a7a-5b34-4eba-a8f6-197ae5fc47a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3360 images belonging to 2 classes.\n",
      "Found 420 images belonging to 2 classes.\n",
      "Found 420 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    os.path.join(m, 'train'),\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    os.path.join(m, 'val'),\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    os.path.join(m, 'test'),\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False  # Don't shuffle test data for evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9cd0b6-d94e-4608-b841-6e4d2a38aff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  6/105 [>.............................] - ETA: 2:38:10 - loss: 0.5144 - accuracy: 0.7292"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=len(validation_generator),\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c798fb9-7759-47e2-ae57-ce8e311ca90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_generator, steps=len(test_generator))\n",
    "print(f'Test accuracy: {test_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cddb851-a492-4a90-af94-e39fe61e3fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for the test data\n",
    "y_true = test_generator.classes\n",
    "y_pred = model.predict(test_generator)\n",
    "y_pred_classes = tf.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c982ae97-9d60-4d13-bc4e-6106ec36195c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification report, confusion matrix, precision, recall, and F1-score\n",
    "class_labels = list(test_generator.class_indices.keys())\n",
    "print(classification_report(y_true, y_pred_classes, target_names=class_labels))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80352c2-b256-44ea-91db-051749ece061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have already trained the model and have the 'history' object\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0860d6db-9cd7-4117-be00-1aba812e96c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "# Save the model in HDF5 format\n",
    "model.save(\"D:\\CAPSTONE 400A\\model\\densetry.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c5af9c-26dc-4bc8-b6cb-2cf60fd8bf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model in Pickle format\n",
    "with open(\"D:\\CAPSTONE 400A\\model\\densetry.pkl\", 'wb') as pickle_file:\n",
    "    pickle.dump(model, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb073171-7334-4830-81c7-8b0230a424f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c52782e-c81e-414a-9d70-8c5aaa686aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
