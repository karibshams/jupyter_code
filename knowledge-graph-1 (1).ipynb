{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-04T13:10:17.506351Z",
     "iopub.status.busy": "2024-12-04T13:10:17.505956Z",
     "iopub.status.idle": "2024-12-04T13:10:23.661054Z",
     "shell.execute_reply": "2024-12-04T13:10:23.659968Z",
     "shell.execute_reply.started": "2024-12-04T13:10:17.506314Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T13:11:01.334524Z",
     "iopub.status.busy": "2024-12-04T13:11:01.334119Z",
     "iopub.status.idle": "2024-12-04T13:11:01.806982Z",
     "shell.execute_reply": "2024-12-04T13:11:01.806268Z",
     "shell.execute_reply.started": "2024-12-04T13:11:01.334490Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = '/kaggle/input/product-caategory/d1.csv'  # Update the path to your dataset\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T13:11:03.697106Z",
     "iopub.status.busy": "2024-12-04T13:11:03.696255Z",
     "iopub.status.idle": "2024-12-04T13:11:03.707536Z",
     "shell.execute_reply": "2024-12-04T13:11:03.706578Z",
     "shell.execute_reply.started": "2024-12-04T13:11:03.697056Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Review</th>\n",
       "      <th>Product Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Redmi 12C (4/128GB)</td>\n",
       "      <td>Positive</td>\n",
       "      <td>‡¶Ö‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£ ‡¶´‡ßã‡¶®‡•§‡¶Ö‡¶®‡ßá‡¶ï ‡¶™‡¶õ‡¶®‡ßç‡¶¶ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§‡¶è‡¶ï‡¶¶‡¶Æ ‡¶Ö‡¶•‡ßá‡¶®‡¶ü‡¶ø‡¶ï ‡¶∂‡¶æ‡¶ì...</td>\n",
       "      <td>Smart Phones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Redmi 12C (4/128GB)</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Phone is good according to my uses, Upgraded f...</td>\n",
       "      <td>Smart Phones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Redmi 12C (4/128GB)</td>\n",
       "      <td>Positive</td>\n",
       "      <td>‡¶Ö‡¶≤‡ßç‡¶™ ‡¶¶‡¶æ‡¶Æ‡ßá ‡¶¶‡¶æ‡¶∞‡ßÅ‡¶® ‡¶è‡¶ï‡¶ü‡¶æ ‡¶∏‡ßç‡¶Æ‡¶æ‡¶∞‡ßç‡¶ü‡¶´‡ßã‡¶® üíô</td>\n",
       "      <td>Smart Phones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Redmi 12C (4/128GB)</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Super Fast Delivery ,11200 TK te pailam</td>\n",
       "      <td>Smart Phones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Redmi 12C (4/128GB)</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Delay Delivery... Good Product.</td>\n",
       "      <td>Smart Phones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78125</th>\n",
       "      <td>Baseus Bipow Digital Display 20W 10000mAh Powe...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>A good one</td>\n",
       "      <td>Power Bank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78126</th>\n",
       "      <td>Baseus Bipow Digital Display 20W 10000mAh Powe...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Overall the product was good. Thanks, Pickaboo...</td>\n",
       "      <td>Power Bank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78127</th>\n",
       "      <td>Baseus Bipow Digital Display 15W 10000mAh Powe...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>This is a very good powerbank in this price po...</td>\n",
       "      <td>Power Bank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78128</th>\n",
       "      <td>Baseus Bipow Digital Display 15W 10000mAh Powe...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>good for long lasting but after 2 year it's fa...</td>\n",
       "      <td>Power Bank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78129</th>\n",
       "      <td>Baseus Bipow Digital Display 15W 10000mAh Powe...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Product Is Original. I Tested &amp; Confirmed Capa...</td>\n",
       "      <td>Power Bank</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78130 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Product Name Sentiment  \\\n",
       "0                                    Redmi 12C (4/128GB)  Positive   \n",
       "1                                    Redmi 12C (4/128GB)  Positive   \n",
       "2                                    Redmi 12C (4/128GB)  Positive   \n",
       "3                                    Redmi 12C (4/128GB)  Positive   \n",
       "4                                    Redmi 12C (4/128GB)  Positive   \n",
       "...                                                  ...       ...   \n",
       "78125  Baseus Bipow Digital Display 20W 10000mAh Powe...  Positive   \n",
       "78126  Baseus Bipow Digital Display 20W 10000mAh Powe...  Positive   \n",
       "78127  Baseus Bipow Digital Display 15W 10000mAh Powe...  Positive   \n",
       "78128  Baseus Bipow Digital Display 15W 10000mAh Powe...  Positive   \n",
       "78129  Baseus Bipow Digital Display 15W 10000mAh Powe...  Positive   \n",
       "\n",
       "                                                  Review Product Category  \n",
       "0      ‡¶Ö‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£ ‡¶´‡ßã‡¶®‡•§‡¶Ö‡¶®‡ßá‡¶ï ‡¶™‡¶õ‡¶®‡ßç‡¶¶ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§‡¶è‡¶ï‡¶¶‡¶Æ ‡¶Ö‡¶•‡ßá‡¶®‡¶ü‡¶ø‡¶ï ‡¶∂‡¶æ‡¶ì...     Smart Phones  \n",
       "1      Phone is good according to my uses, Upgraded f...     Smart Phones  \n",
       "2                      ‡¶Ö‡¶≤‡ßç‡¶™ ‡¶¶‡¶æ‡¶Æ‡ßá ‡¶¶‡¶æ‡¶∞‡ßÅ‡¶® ‡¶è‡¶ï‡¶ü‡¶æ ‡¶∏‡ßç‡¶Æ‡¶æ‡¶∞‡ßç‡¶ü‡¶´‡ßã‡¶® üíô     Smart Phones  \n",
       "3                Super Fast Delivery ,11200 TK te pailam     Smart Phones  \n",
       "4                        Delay Delivery... Good Product.     Smart Phones  \n",
       "...                                                  ...              ...  \n",
       "78125                                         A good one       Power Bank  \n",
       "78126  Overall the product was good. Thanks, Pickaboo...       Power Bank  \n",
       "78127  This is a very good powerbank in this price po...       Power Bank  \n",
       "78128  good for long lasting but after 2 year it's fa...       Power Bank  \n",
       "78129  Product Is Original. I Tested & Confirmed Capa...       Power Bank  \n",
       "\n",
       "[78130 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T09:27:35.251970Z",
     "iopub.status.busy": "2024-12-01T09:27:35.251490Z",
     "iopub.status.idle": "2024-12-01T09:28:53.985076Z",
     "shell.execute_reply": "2024-12-01T09:28:53.984089Z",
     "shell.execute_reply.started": "2024-12-01T09:27:35.251938Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daf6a3c2142647bcb74b0b0936c5926e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "185b3bc27d854e6fbcfe7724c635a962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/872k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79067a95e8bb463a9dbb04c0d17993ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.72M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2e699818eb34e748e68e01b7449be0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2837: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c6264b62734047a58e565d89eafce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/672M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(105879, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_path = '/kaggle/input/sentiment/d3_sentiment.csv'  # Update the path to your dataset\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Combine the 'Product name', 'Emotion', and 'Review' columns into a single text input\n",
    "data['Combined'] = data['Product Name'] + \" \" + data['Review']\n",
    "texts = data['Combined']\n",
    "sentiments = data['Sentiment']  # Assuming sentiments are labeled as 'Positive', 'Negative'\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "\n",
    "def encode_data(tokenizer, texts, max_length=256):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for text in texts:\n",
    "        encoded_text = tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=max_length,\n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "            pad_to_max_length=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        input_ids.append(encoded_text['input_ids'])\n",
    "        attention_masks.append(encoded_text['attention_mask'])\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "input_ids, attention_masks = encode_data(tokenizer, texts)\n",
    "\n",
    "# Convert sentiment labels to numerical form\n",
    "label_map = {'Positive': 1, 'Negative': 0}  # Update this map based on your dataset's labels\n",
    "labels = torch.tensor(sentiments.map(label_map).values)\n",
    "\n",
    "# Split data into train and validation sets\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=2018, test_size=0.2)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels, random_state=2018, test_size=0.2)\n",
    "\n",
    "# Create DataLoader\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=32)\n",
    "\n",
    "# Load the BERT model\n",
    "model_name = 'bert-base-multilingual-uncased'\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T09:28:53.986782Z",
     "iopub.status.busy": "2024-12-01T09:28:53.986457Z",
     "iopub.status.idle": "2024-12-01T17:07:49.229724Z",
     "shell.execute_reply": "2024-12-01T17:07:49.228671Z",
     "shell.execute_reply.started": "2024-12-01T09:28:53.986754Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n",
      "  Batch   100  of  1,954.    Elapsed: 0:02:12.\n",
      "  Batch   200  of  1,954.    Elapsed: 0:04:33.\n",
      "  Batch   300  of  1,954.    Elapsed: 0:06:54.\n",
      "  Batch   400  of  1,954.    Elapsed: 0:09:15.\n",
      "  Batch   500  of  1,954.    Elapsed: 0:11:36.\n",
      "  Batch   600  of  1,954.    Elapsed: 0:13:57.\n",
      "  Batch   700  of  1,954.    Elapsed: 0:16:19.\n",
      "  Batch   800  of  1,954.    Elapsed: 0:18:40.\n",
      "  Batch   900  of  1,954.    Elapsed: 0:21:01.\n",
      "  Batch 1,000  of  1,954.    Elapsed: 0:23:22.\n",
      "  Batch 1,100  of  1,954.    Elapsed: 0:25:44.\n",
      "  Batch 1,200  of  1,954.    Elapsed: 0:28:05.\n",
      "  Batch 1,300  of  1,954.    Elapsed: 0:30:26.\n",
      "  Batch 1,400  of  1,954.    Elapsed: 0:32:48.\n",
      "  Batch 1,500  of  1,954.    Elapsed: 0:35:09.\n",
      "  Batch 1,600  of  1,954.    Elapsed: 0:37:30.\n",
      "  Batch 1,700  of  1,954.    Elapsed: 0:39:51.\n",
      "  Batch 1,800  of  1,954.    Elapsed: 0:42:12.\n",
      "  Batch 1,900  of  1,954.    Elapsed: 0:44:33.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:45:48\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "  Batch   100  of  1,954.    Elapsed: 0:02:21.\n",
      "  Batch   200  of  1,954.    Elapsed: 0:04:42.\n",
      "  Batch   300  of  1,954.    Elapsed: 0:07:04.\n",
      "  Batch   400  of  1,954.    Elapsed: 0:09:25.\n",
      "  Batch   500  of  1,954.    Elapsed: 0:11:47.\n",
      "  Batch   600  of  1,954.    Elapsed: 0:14:08.\n",
      "  Batch   700  of  1,954.    Elapsed: 0:16:29.\n",
      "  Batch   800  of  1,954.    Elapsed: 0:18:50.\n",
      "  Batch   900  of  1,954.    Elapsed: 0:21:11.\n",
      "  Batch 1,000  of  1,954.    Elapsed: 0:23:32.\n",
      "  Batch 1,100  of  1,954.    Elapsed: 0:25:53.\n",
      "  Batch 1,200  of  1,954.    Elapsed: 0:28:14.\n",
      "  Batch 1,300  of  1,954.    Elapsed: 0:30:35.\n",
      "  Batch 1,400  of  1,954.    Elapsed: 0:32:57.\n",
      "  Batch 1,500  of  1,954.    Elapsed: 0:35:18.\n",
      "  Batch 1,600  of  1,954.    Elapsed: 0:37:39.\n",
      "  Batch 1,700  of  1,954.    Elapsed: 0:40:00.\n",
      "  Batch 1,800  of  1,954.    Elapsed: 0:42:21.\n",
      "  Batch 1,900  of  1,954.    Elapsed: 0:44:42.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epoch took: 0:45:57\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "  Batch   100  of  1,954.    Elapsed: 0:02:21.\n",
      "  Batch   200  of  1,954.    Elapsed: 0:04:42.\n",
      "  Batch   300  of  1,954.    Elapsed: 0:07:03.\n",
      "  Batch   400  of  1,954.    Elapsed: 0:09:24.\n",
      "  Batch   500  of  1,954.    Elapsed: 0:11:46.\n",
      "  Batch   600  of  1,954.    Elapsed: 0:14:07.\n",
      "  Batch   700  of  1,954.    Elapsed: 0:16:28.\n",
      "  Batch   800  of  1,954.    Elapsed: 0:18:49.\n",
      "  Batch   900  of  1,954.    Elapsed: 0:21:10.\n",
      "  Batch 1,000  of  1,954.    Elapsed: 0:23:31.\n",
      "  Batch 1,100  of  1,954.    Elapsed: 0:25:52.\n",
      "  Batch 1,200  of  1,954.    Elapsed: 0:28:13.\n",
      "  Batch 1,300  of  1,954.    Elapsed: 0:30:34.\n",
      "  Batch 1,400  of  1,954.    Elapsed: 0:32:55.\n",
      "  Batch 1,500  of  1,954.    Elapsed: 0:35:16.\n",
      "  Batch 1,600  of  1,954.    Elapsed: 0:37:36.\n",
      "  Batch 1,700  of  1,954.    Elapsed: 0:39:57.\n",
      "  Batch 1,800  of  1,954.    Elapsed: 0:42:18.\n",
      "  Batch 1,900  of  1,954.    Elapsed: 0:44:39.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epoch took: 0:45:54\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "  Batch   100  of  1,954.    Elapsed: 0:02:21.\n",
      "  Batch   200  of  1,954.    Elapsed: 0:04:42.\n",
      "  Batch   300  of  1,954.    Elapsed: 0:07:03.\n",
      "  Batch   400  of  1,954.    Elapsed: 0:09:24.\n",
      "  Batch   500  of  1,954.    Elapsed: 0:11:45.\n",
      "  Batch   600  of  1,954.    Elapsed: 0:14:06.\n",
      "  Batch   700  of  1,954.    Elapsed: 0:16:27.\n",
      "  Batch   800  of  1,954.    Elapsed: 0:18:47.\n",
      "  Batch   900  of  1,954.    Elapsed: 0:21:08.\n",
      "  Batch 1,000  of  1,954.    Elapsed: 0:23:29.\n",
      "  Batch 1,100  of  1,954.    Elapsed: 0:25:50.\n",
      "  Batch 1,200  of  1,954.    Elapsed: 0:28:11.\n",
      "  Batch 1,300  of  1,954.    Elapsed: 0:30:31.\n",
      "  Batch 1,400  of  1,954.    Elapsed: 0:32:52.\n",
      "  Batch 1,500  of  1,954.    Elapsed: 0:35:13.\n",
      "  Batch 1,600  of  1,954.    Elapsed: 0:37:34.\n",
      "  Batch 1,700  of  1,954.    Elapsed: 0:39:55.\n",
      "  Batch 1,800  of  1,954.    Elapsed: 0:42:16.\n",
      "  Batch 1,900  of  1,954.    Elapsed: 0:44:37.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epoch took: 0:45:52\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n",
      "  Batch   100  of  1,954.    Elapsed: 0:02:21.\n",
      "  Batch   200  of  1,954.    Elapsed: 0:04:42.\n",
      "  Batch   300  of  1,954.    Elapsed: 0:07:03.\n",
      "  Batch   400  of  1,954.    Elapsed: 0:09:24.\n",
      "  Batch   500  of  1,954.    Elapsed: 0:11:45.\n",
      "  Batch   600  of  1,954.    Elapsed: 0:14:06.\n",
      "  Batch   700  of  1,954.    Elapsed: 0:16:27.\n",
      "  Batch   800  of  1,954.    Elapsed: 0:18:48.\n",
      "  Batch   900  of  1,954.    Elapsed: 0:21:09.\n",
      "  Batch 1,000  of  1,954.    Elapsed: 0:23:30.\n",
      "  Batch 1,100  of  1,954.    Elapsed: 0:25:51.\n",
      "  Batch 1,200  of  1,954.    Elapsed: 0:28:12.\n",
      "  Batch 1,300  of  1,954.    Elapsed: 0:30:33.\n",
      "  Batch 1,400  of  1,954.    Elapsed: 0:32:54.\n",
      "  Batch 1,500  of  1,954.    Elapsed: 0:35:15.\n",
      "  Batch 1,600  of  1,954.    Elapsed: 0:37:36.\n",
      "  Batch 1,700  of  1,954.    Elapsed: 0:39:57.\n",
      "  Batch 1,800  of  1,954.    Elapsed: 0:42:18.\n",
      "  Batch 1,900  of  1,954.    Elapsed: 0:44:39.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epoch took: 0:45:54\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "  Batch   100  of  1,954.    Elapsed: 0:02:21.\n",
      "  Batch   200  of  1,954.    Elapsed: 0:04:42.\n",
      "  Batch   300  of  1,954.    Elapsed: 0:07:03.\n",
      "  Batch   400  of  1,954.    Elapsed: 0:09:24.\n",
      "  Batch   500  of  1,954.    Elapsed: 0:11:45.\n",
      "  Batch   600  of  1,954.    Elapsed: 0:14:05.\n",
      "  Batch   700  of  1,954.    Elapsed: 0:16:27.\n",
      "  Batch   800  of  1,954.    Elapsed: 0:18:47.\n",
      "  Batch   900  of  1,954.    Elapsed: 0:21:08.\n",
      "  Batch 1,000  of  1,954.    Elapsed: 0:23:30.\n",
      "  Batch 1,100  of  1,954.    Elapsed: 0:25:50.\n",
      "  Batch 1,200  of  1,954.    Elapsed: 0:28:11.\n",
      "  Batch 1,300  of  1,954.    Elapsed: 0:30:32.\n",
      "  Batch 1,400  of  1,954.    Elapsed: 0:32:53.\n",
      "  Batch 1,500  of  1,954.    Elapsed: 0:35:14.\n",
      "  Batch 1,600  of  1,954.    Elapsed: 0:37:35.\n",
      "  Batch 1,700  of  1,954.    Elapsed: 0:39:56.\n",
      "  Batch 1,800  of  1,954.    Elapsed: 0:42:17.\n",
      "  Batch 1,900  of  1,954.    Elapsed: 0:44:38.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epoch took: 0:45:54\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "  Batch   100  of  1,954.    Elapsed: 0:02:21.\n",
      "  Batch   200  of  1,954.    Elapsed: 0:04:42.\n",
      "  Batch   300  of  1,954.    Elapsed: 0:07:03.\n",
      "  Batch   400  of  1,954.    Elapsed: 0:09:24.\n",
      "  Batch   500  of  1,954.    Elapsed: 0:11:45.\n",
      "  Batch   600  of  1,954.    Elapsed: 0:14:06.\n",
      "  Batch   700  of  1,954.    Elapsed: 0:16:27.\n",
      "  Batch   800  of  1,954.    Elapsed: 0:18:48.\n",
      "  Batch   900  of  1,954.    Elapsed: 0:21:09.\n",
      "  Batch 1,000  of  1,954.    Elapsed: 0:23:30.\n",
      "  Batch 1,100  of  1,954.    Elapsed: 0:25:51.\n",
      "  Batch 1,200  of  1,954.    Elapsed: 0:28:12.\n",
      "  Batch 1,300  of  1,954.    Elapsed: 0:30:33.\n",
      "  Batch 1,400  of  1,954.    Elapsed: 0:32:54.\n",
      "  Batch 1,500  of  1,954.    Elapsed: 0:35:15.\n",
      "  Batch 1,600  of  1,954.    Elapsed: 0:37:35.\n",
      "  Batch 1,700  of  1,954.    Elapsed: 0:39:56.\n",
      "  Batch 1,800  of  1,954.    Elapsed: 0:42:17.\n",
      "  Batch 1,900  of  1,954.    Elapsed: 0:44:38.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epoch took: 0:45:53\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "  Batch   100  of  1,954.    Elapsed: 0:02:21.\n",
      "  Batch   200  of  1,954.    Elapsed: 0:04:42.\n",
      "  Batch   300  of  1,954.    Elapsed: 0:07:03.\n",
      "  Batch   400  of  1,954.    Elapsed: 0:09:24.\n",
      "  Batch   500  of  1,954.    Elapsed: 0:11:45.\n",
      "  Batch   600  of  1,954.    Elapsed: 0:14:06.\n",
      "  Batch   700  of  1,954.    Elapsed: 0:16:27.\n",
      "  Batch   800  of  1,954.    Elapsed: 0:18:48.\n",
      "  Batch   900  of  1,954.    Elapsed: 0:21:09.\n",
      "  Batch 1,000  of  1,954.    Elapsed: 0:23:30.\n",
      "  Batch 1,100  of  1,954.    Elapsed: 0:25:51.\n",
      "  Batch 1,200  of  1,954.    Elapsed: 0:28:12.\n",
      "  Batch 1,300  of  1,954.    Elapsed: 0:30:33.\n",
      "  Batch 1,400  of  1,954.    Elapsed: 0:32:54.\n",
      "  Batch 1,500  of  1,954.    Elapsed: 0:35:15.\n",
      "  Batch 1,600  of  1,954.    Elapsed: 0:37:35.\n",
      "  Batch 1,700  of  1,954.    Elapsed: 0:39:56.\n",
      "  Batch 1,800  of  1,954.    Elapsed: 0:42:17.\n",
      "  Batch 1,900  of  1,954.    Elapsed: 0:44:38.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epoch took: 0:45:53\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n",
      "  Batch   100  of  1,954.    Elapsed: 0:02:21.\n",
      "  Batch   200  of  1,954.    Elapsed: 0:04:42.\n",
      "  Batch   300  of  1,954.    Elapsed: 0:07:03.\n",
      "  Batch   400  of  1,954.    Elapsed: 0:09:24.\n",
      "  Batch   500  of  1,954.    Elapsed: 0:11:45.\n",
      "  Batch   600  of  1,954.    Elapsed: 0:14:07.\n",
      "  Batch   700  of  1,954.    Elapsed: 0:16:27.\n",
      "  Batch   800  of  1,954.    Elapsed: 0:18:49.\n",
      "  Batch   900  of  1,954.    Elapsed: 0:21:10.\n",
      "  Batch 1,000  of  1,954.    Elapsed: 0:23:31.\n",
      "  Batch 1,100  of  1,954.    Elapsed: 0:25:52.\n",
      "  Batch 1,200  of  1,954.    Elapsed: 0:28:13.\n",
      "  Batch 1,300  of  1,954.    Elapsed: 0:30:35.\n",
      "  Batch 1,400  of  1,954.    Elapsed: 0:32:55.\n",
      "  Batch 1,500  of  1,954.    Elapsed: 0:35:17.\n",
      "  Batch 1,600  of  1,954.    Elapsed: 0:37:37.\n",
      "  Batch 1,700  of  1,954.    Elapsed: 0:39:58.\n",
      "  Batch 1,800  of  1,954.    Elapsed: 0:42:19.\n",
      "  Batch 1,900  of  1,954.    Elapsed: 0:44:40.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epoch took: 0:45:56\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n",
      "  Batch   100  of  1,954.    Elapsed: 0:02:21.\n",
      "  Batch   200  of  1,954.    Elapsed: 0:04:42.\n",
      "  Batch   300  of  1,954.    Elapsed: 0:07:03.\n",
      "  Batch   400  of  1,954.    Elapsed: 0:09:24.\n",
      "  Batch   500  of  1,954.    Elapsed: 0:11:45.\n",
      "  Batch   600  of  1,954.    Elapsed: 0:14:06.\n",
      "  Batch   700  of  1,954.    Elapsed: 0:16:27.\n",
      "  Batch   800  of  1,954.    Elapsed: 0:18:48.\n",
      "  Batch   900  of  1,954.    Elapsed: 0:21:09.\n",
      "  Batch 1,000  of  1,954.    Elapsed: 0:23:30.\n",
      "  Batch 1,100  of  1,954.    Elapsed: 0:25:51.\n",
      "  Batch 1,200  of  1,954.    Elapsed: 0:28:12.\n",
      "  Batch 1,300  of  1,954.    Elapsed: 0:30:33.\n",
      "  Batch 1,400  of  1,954.    Elapsed: 0:32:54.\n",
      "  Batch 1,500  of  1,954.    Elapsed: 0:35:15.\n",
      "  Batch 1,600  of  1,954.    Elapsed: 0:37:36.\n",
      "  Batch 1,700  of  1,954.    Elapsed: 0:39:57.\n",
      "  Batch 1,800  of  1,954.    Elapsed: 0:42:18.\n",
      "  Batch 1,900  of  1,954.    Elapsed: 0:44:39.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epoch took: 0:45:54\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Check if a GPU is available and use it; otherwise, use the CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the chosen device\n",
    "model = model.to(device)\n",
    "\n",
    "# Training loop parameters\n",
    "epochs = 10\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\n",
    "# Total number of training steps\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Lists to store loss values\n",
    "train_loss_values = []\n",
    "val_loss_values = []\n",
    "test_loss_values = []  # New list for test loss\n",
    "\n",
    "# Function to calculate elapsed time\n",
    "def format_time(elapsed):\n",
    "    return str(datetime.timedelta(seconds=int(round((elapsed)))))\n",
    "\n",
    "# Function to evaluate loss for validation or test set\n",
    "def evaluate_loss(model, dataloader, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "\n",
    "# Main training loop\n",
    "for epoch_i in range(0, epochs):\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 100 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    train_loss_values.append(avg_train_loss)\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(training_time))\n",
    "\n",
    "    # Validation loss after each epoch\n",
    "    print(\"Running Validation...\")\n",
    "    t0 = time.time()\n",
    "    val_loss = evaluate_loss(model, validation_dataloader, device)\n",
    "    val_loss_values.append(val_loss)\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(val_loss))\n",
    "    print(\"  Validation epoch took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "# Testing loss after training completes\n",
    "print(\"\\nRunning Testing...\")\n",
    "test_loss = evaluate_loss(model, test_dataloader, device)  # Ensure test_dataloader is defined\n",
    "test_loss_values.append(test_loss)\n",
    "print(\"  Testing Loss: {0:.2f}\".format(test_loss))\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T17:07:49.232097Z",
     "iopub.status.busy": "2024-12-01T17:07:49.231603Z",
     "iopub.status.idle": "2024-12-01T17:11:56.095359Z",
     "shell.execute_reply": "2024-12-01T17:11:56.094275Z",
     "shell.execute_reply.started": "2024-12-01T17:07:49.232066Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9999\n",
      "Precision: 0.9999\n",
      "Recall: 0.9999\n",
      "F1 Score: 0.9999\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      2169\n",
      "           1       1.00      1.00      1.00     13457\n",
      "\n",
      "    accuracy                           1.00     15626\n",
      "   macro avg       1.00      1.00      1.00     15626\n",
      "weighted avg       1.00      1.00      1.00     15626\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Function to get predictions from the model\n",
    "def get_predictions(model, dataloader, device):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        predictions.append(np.argmax(logits, axis=1))\n",
    "        true_labels.append(label_ids)\n",
    "\n",
    "    return np.concatenate(predictions), np.concatenate(true_labels)\n",
    "\n",
    "# Get predictions and true labels for the test set\n",
    "test_preds, test_labels = get_predictions(model, test_dataloader, device)\n",
    "\n",
    "# Step 2: Calculate Accuracy\n",
    "test_accuracy = accuracy_score(test_labels, test_preds)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "# Step 3: Generate Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_labels, test_preds, target_names=['Negative', 'Positive']))\n",
    "\n",
    "# Step 4: Confusion Matrix\n",
    "conf_matrix = confusion_matrix(test_labels, test_preds)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Step 5: Plot Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6185576,
     "sourceId": 10041270,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6196564,
     "sourceId": 10056199,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
